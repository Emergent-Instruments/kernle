---
title: Sync
description: "API endpoints for memory synchronization"
---

# Sync Endpoints

Endpoints for synchronizing memory between local and cloud storage.

## Sync Architecture

Kernle uses a **local-first** sync model:

1. All changes write to local SQLite first
2. Changes queue for sync
3. Push sends queued changes to cloud
4. Pull retrieves remote changes

### Conflict Resolution

Kernle uses a hybrid approach for conflicts:

- **Scalar fields** (title, content, confidence, etc.): Last-write-wins based on `local_updated_at` timestamp
- **Array fields** (tags, lessons, etc.): Set union merge preserves items from both versions

<Info>
**Why merge arrays?** Tags, lessons, and other array fields often accumulate independently on different devices. Merging ensures you never lose data that was added on one device while working offline on another.
</Info>

#### Merged Array Fields by Table

| Table | Merged Fields |
|-------|---------------|
| `episodes` | `lessons`, `tags`, `emotional_tags`, `source_episodes`, `derived_from`, `context_tags` |
| `beliefs` | `source_episodes`, `derived_from`, `context_tags` |
| `notes` | `tags`, `source_episodes`, `derived_from`, `context_tags` |
| `drives` | `focus_areas`, `source_episodes`, `derived_from`, `context_tags` |
| `playbooks` | `trigger_conditions`, `failure_modes`, `recovery_steps`, `source_episodes`, `tags` |

<Warning>
Arrays are capped at **500 items** after merging to prevent resource exhaustion.
</Warning>

---

## Push Changes

Push local changes to cloud.

```http
POST /sync/push
```

### Headers

```
Authorization: Bearer knl_sk_your-api-key
Content-Type: application/json
```

### Request Body

```json
{
  "agent_id": "claire",
  "operations": [
    {
      "table": "episodes",
      "record_id": "ep_abc123",
      "operation": "upsert",
      "data": {
        "objective": "Debugged API issue",
        "outcome": "success",
        "outcome_type": "success",
        "lessons": ["Check logs first"],
        "created_at": "2024-01-15T10:30:00Z",
        "local_updated_at": "2024-01-15T10:30:00Z"
      }
    }
  ],
  "limit": 100
}
```

### Response

```json
{
  "success": true,
  "data": {
    "pushed": 5,
    "failed": 0,
    "remaining": 0,
    "results": [
      {
        "record_id": "ep_abc123",
        "status": "synced",
        "remote_id": "cloud_ep_abc123"
      }
    ]
  }
}
```

### CLI Equivalent

```bash
kernle -a claire sync push
```

---

## Pull Changes

Pull remote changes to local.

```http
POST /sync/pull
```

### Headers

```
Authorization: Bearer knl_sk_your-api-key
Content-Type: application/json
```

### Request Body

```json
{
  "agent_id": "claire",
  "since": "2024-01-15T00:00:00Z",
  "full": false
}
```

| Field | Type | Description |
|-------|------|-------------|
| `agent_id` | string | Required. Agent identifier |
| `since` | ISO timestamp | Pull changes after this time (default: last pull time) |
| `full` | boolean | If true, pull all records regardless of timestamp |

### Response

```json
{
  "success": true,
  "data": {
    "pulled": 12,
    "tables": {
      "episodes": 5,
      "beliefs": 3,
      "notes": 4
    },
    "last_sync": "2024-01-15T10:30:00Z"
  }
}
```

### CLI Equivalent

```bash
# Incremental pull
kernle -a claire sync pull

# Full pull
kernle -a claire sync pull --full
```

---

## Full Sync

Perform bidirectional sync (pull then push).

```http
POST /sync/full
```

### Headers

```
Authorization: Bearer knl_sk_your-api-key
Content-Type: application/json
```

### Request Body

```json
{
  "agent_id": "claire"
}
```

### Response

```json
{
  "success": true,
  "data": {
    "pull": {
      "records": 8,
      "tables": ["episodes", "beliefs"]
    },
    "push": {
      "records": 3,
      "tables": ["notes", "raw_entries"]
    },
    "conflicts_resolved": 0,
    "sync_completed_at": "2024-01-15T10:30:00Z"
  }
}
```

### CLI Equivalent

```bash
kernle -a claire sync full
```

---

## Supported Tables

The sync API supports these memory tables:

| Table | Description |
|-------|-------------|
| `episodes` | Autobiographical experiences |
| `beliefs` | Semantic knowledge with confidence |
| `notes` | Quick captures (decisions, insights) |
| `agent_values` | Core principles |
| `goals` | Active objectives |
| `drives` | Intrinsic motivations |
| `relationships` | Models of other entities |
| `playbooks` | Procedural memory |
| `raw_entries` | Unprocessed captures |
| `checkpoints` | Saved state |

---

## Queue Entry Format

Locally, changes are queued in the `sync_queue` table before being pushed:

```python
{
    "table_name": "episodes",     # Target table
    "record_id": "uuid-string",   # Record's unique ID
    "operation": "update",        # Always use "update" for upsert
    "data": {...},                # Record data (JSON)
    "local_updated_at": "ISO8601" # Timestamp for conflict resolution
}
```

<Info>
The client SDK always uses `operation: "update"` internally, regardless of whether the record is new. This provides upsert semantics. External API consumers should do the same — avoid using `"insert"` which may fail if the record already exists.
</Info>

---

## Required Fields

All records must include:
- `id` — UUID string
- `agent_id` — Must match the authenticated agent

Only send fields that exist in the database schema. Unknown fields will cause silent failures with `synced=0`.

### Episode Fields

`id`, `agent_id`, `objective`, `outcome`, `lesson`, `timestamp`, `emotional_valence`, `emotional_arousal`, `emotional_tags`, `confidence`, `source_type`, `derived_from`, `source_episodes`, `context`, `context_tags`, `subject_ids`, `access_grants`, `times_accessed`, `last_accessed`, `is_protected`, `is_forgotten`

### Belief Fields

`id`, `agent_id`, `statement`, `belief_type`, `confidence`, `context`, `context_tags`, `source_type`, `derived_from`, `is_active`, `supersedes`, `superseded_by`, `times_reinforced`

See schema files for complete field lists for all table types.

---

## Retry Behavior

The local client implements resilient sync (v0.2.5+):

- Failed records increment `retry_count`
- Records with 5+ retries are moved to a "dead letter" queue
- Sync continues processing other records after failures
- Dead letter records can be inspected and cleared

```python
# Get failed records
failed = storage.get_failed_sync_records(min_retries=5)

# Clear old failures (> 7 days)
cleared = storage.clear_failed_sync_records(older_than_days=7)
```

---

## Common Issues

### "Database error: operation failed"

**Cause:** Unknown field in data payload.

**Fix:** Only include known schema fields. Check the table schema before pushing.

### Records stuck in queue

**Cause:** Persistent failures (e.g., invalid agent_id).

**Fix:** Check `get_failed_sync_records()` for error details. Clear or fix problematic records.

### Insert vs Update confusion

**Cause:** Using `operation: "insert"` for records that may already exist.

**Fix:** Always use `operation: "update"` for client sync. The backend handles upsert semantics.

---

## Webhook Notifications (Coming Soon)

Configure webhooks to receive notifications when sync events occur:

```json
{
  "webhook_url": "https://your-app.com/kernle-webhook",
  "events": ["sync.completed", "memory.created", "conflict.detected"]
}
```
